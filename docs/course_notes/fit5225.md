Create time: 2023-02-24  Last update: 2023-02-24

# How to use the FIT study Note document
1. download the [markdown file repository](https://github.com/GreenH47/mynote) and  navigate to the `docs` folder
2. view all the markdown files via [Obsidian vault](https://help.obsidian.md/How+to/Working+with+multiple+vaults) that can show the linked section in the note document  ![](../img/5032-20221103.png)  
3. You may find some extra material or program template  repository in the Course Brief introduction for the FIT Note markdown Document (some course don't have )

4. you can view [the web page](https://greenh47.github.io/mynote/) which transfer from MD file online but will lose the extra information or wrong    markdown display

  
# FIT5225 Cloud computing and security Course Brief introduction

![](../img/fit5225-20230226.png)  
![](../img/fit5225-20230226-2.png)  
You need to have a good grasp of computer networks (TCP/IP), be familiar with Linux and its shell  scripting and command line, you need to use python a lot and there are many hands-on activities  in this unit.  


Unit learning Objectives  
1. describe fundamental principles and paradigms of cloud computing;  
2. identify appropriate design choices when developing real-world cloud computing applications;  
3. apply different cloud programming methods and tools;  
4. demonstrate a comprehensive understanding of virtualisation and container technologies;  
5. design and implement a highly scalable cloud-based application;  
6. analyse and evaluate the security of the current cloud services and in-cloud applications.  

[FIT5225 - Cloud computing and security](https://handbook.monash.edu/2023/units/FIT5225?year=2023) done for the year 2023. (Semester 1)  

# week 1 Introduction to Cloud Computing
## Lecture
### Learning Objectives:  
describe fundamental principles and paradigms of cloud computing  

identify appropriate design choices when developing real-world cloud computing applications  

Summary  
â€¢ Computer Networks vs Distributed Systems  
â€¢ Distributed Systems Challenges: heterogeneity, openness, security, scalability, failure handling,  concurrency, transparency, Quality of Service.  
â€¢ Cloud computing is all about delivering computing services over the Internet.  
â€¢ Technologies significantly contributed to make cloud computing viable: Grid computing,  webservices, virtualization, autonomic computing  
â€¢ Cloud Deployment Models: private, community, public, hybrid  
â€¢ Cloud Service Models: IaaS, PaaS, SaaS  
â€¢ Cloud and distributed computing skill is at the top of the most demanded job skills right now.
### Computer Networks vs Distributed Systems
+ A Computer Network: Is a collection of spatially separated, interconnected computers that  exchange messages based on specific protocols. Computers are addressed by IP addresses.  
+ A Distributed System: Multiple computers on the network working together as a system. The spatial  separation of computers and communication aspects are hidden from users
### Distributed System Challenges
1. Heterogeneity-  use hardware and software resources of  varying characteristics (how to solve: Using standard protocol;  agreed upon message formats; API; Middleware; )   
2. Openness: ability of extending the system in  different ways by adding hardware or software resources (key interfaces; uniform communication mechanism)  
3. Security: aspects of security (Confidentiality; Integrity;  Availability); Security Mechanisms(Encryption; Authentication; Authorization)  
4. Scalability: can handle the growth of the number of users.  
5. Concurrency å¹¶å‘: access the same resource at the same time;
6. Failure Handling: Detecting, Masking, Tolerating, Recovery, Redundancy  
7. Transparency
8. Quality of Service (QoS) non-functional properties of systems that affect QoS are: Reliability , Security, Performance, Adaptability, Availability 
### Client-Server distributed architectures
![](../img/fit5225-20230312-33.png)
Clients invoke services in servers and results are returned. Servers in turn can become clients to other services
### Peer-to-Peer
![](../img/fit5225-20230312-35.png)
Each process in the systems plays a similar role interacting cooperatively as peers  (playing the roles of client and server simultaneously)
### Cloud Computing
An IT paradigm that enables access to shared pools of configurable system resources in form of services that can be rapidly provisioned with minimal management effort, often over the Internet.  d<mark style="background: #3CB371;">elivering computing services over the Internet.</mark>
Avoid expensive up-front investments of establishing their own infrastructure  
Cloud is the â€œinvisibleâ€ backend to many application  
### Cloud Service Models
![](../img/fit5225-20230312-41.png)
#### Software as a Service (SaaS)
provides <mark style="background: #0000CD;">applications and software to the customer in utility</mark>  based model which is accessible from a thin client interface  such as a Web browser  â€¢ Salesforce.com
#### Platform as a Service (PaaS)
provides <mark style="background: #ff0000;">programming languages and tools to deploy  application</mark> onto the cloud infrastructure  Google App Engine
#### Infrastructure as a Service (IaaS)
provides capabilities for the customers to <mark style="background: #3CB371;">provision  computational resources such as processing, storage,  network, and other fundamental computing </mark>resources Virtual  Machines(VMs)/Containers  â€¢ Example: Amazon EC2/



### Grid Computing
computer cluster is a <mark style="background: #0000CD;">set of computers connected by a local area network (LAN) that work  together</mark> can be viewed as a single system  
Grid computing: A type of parallel and distributed system to share compute and storage resources  distributed across different administrative domains
![](../img/fit5225-20230312-36.png)
### Web service
A Web service is a s<mark style="background: #ff0000;">oftware system designed to support interoperable machine-to-machine  interaction over a network</mark> using HTTP and technologies such as XML, SOAP, WSDL, and UDDI. 
![](../img/fit5225-20230312-37.png)
### Hardware Virtualization
![](../img/fit5225-20230312-39.png)
hides the physical characteristics of a computing platform from the users, presenting  instead an abstract computing platform
### Autonomic Computing
Autonomic computing refers to the <mark style="background: #D2691E;">self-managing</mark> characteristics of distributed  computing resources, <mark style="background: #D2691E;">adapting to unpredictable changes</mark> while hiding intrinsic complexity to  operators and users
## quiz
![](../img/screencapture-flux-qa-2023-02-27-11_41_18.png)
![](../img/fit5225-20230312-6.png)  
![](../img/fit5225-20230312-9.png)
![](../img/fit5225-20230312-10.png)
![](../img/fit5225-20230312-11.png)
![](../img/fit5225-20230312-14.png)
![](../img/fit5225-20230312-17.png)
![](../img/fit5225-20230312-21.png)
![](../img/fit5225-20230312-22.png)
![](../img/fit5225-20230312-24.png)
![](../img/fit5225-20230312-25.png)
![](../img/fit5225-20230312-26.png)

## workshop
In this week tutorial classes, you will learn to work with Nectar as an example of a community cloud. You are supposed to run a virtual machine in the Nectar cloud and get familiar with the Nectar dashboard.

Upon successful completion of this tutorial, you will gain the required knowledge to access the Nectar portal, create/modify virtual machines, establish remote connections to your VMs, and perform administration tasks such as installing a web server and opening required ports using security groups.  
### how to set
1. generate ssh key `ssh-keygen`  
2. set instance in nector
3. import your public key to match
4. ssh into your instance `ssh -i <full-path-to-private-key> ubuntu@<IP-address>` 
5. install the latest version of Apache web server `$ sudo apt update && sudo apt install -y apache2 curl`  
6. print the default Apache  landing page `$ curl localhost`


# Week 2 Grid and Cluster Computing
## Learning Objectives:

-   describe fundamental principles and paradigms of cloud computing
-   identify appropriate design choices when developing real-world cloud computing applications
## Reference
Cluster Computing:Â [https://en.wikipedia.org/wiki/Computer_cluster  
](https://en.wikipedia.org/wiki/Computer_cluster)Grid Computing:Â [https://en.wikipedia.org/wiki/Grid_computing](https://en.wikipedia.org/wiki/Grid_computing)
## Lecture
### Learning Outcomes  
1. describe fundamental principles and paradigms of cloud computing;  
2. identify appropriate design choices when developing real-world cloud computing applications;  
### Flynn Matrix
+ Single instruction stream, single data  stream (SISD)  
+ Multiple instruction streams, single data  stream (MISD)  
+ Single instruction stream, multiple data  streams (SIMD)  
+ Multiple instruction streams, multiple data  streams (MIMD
+ ![](../img/fit5225-20230312-43.png)
+ ![](../img/fit5225-20230312-44.png)
### parallel Computing Architectures
é€‰æ‹©å“ªç§ç³»ç»Ÿå–å†³äºåº”ç”¨ç¨‹åºçš„ç‰¹æ€§å’Œè¦æ±‚ï¼Œä»¥åŠç³»ç»Ÿçš„æ€§èƒ½å’Œå¯ä¼¸ç¼©æ€§éœ€æ±‚ã€‚ 
+ UMAç³»ç»Ÿé€‚  åˆè§„æ¨¡è¾ƒå°ã€æ•°æ®å…±äº«éœ€æ±‚è¾ƒå¼ºçš„å¤šå¤„ç†å™¨ç³»ç»Ÿï¼› 
+ SMPç³»ç»Ÿåˆ™æ›´é€‚åˆè§„æ¨¡è¾ƒå¤§ã€æ•°æ®å…±äº«éœ€æ±‚ ç›¸å¯¹è¾ƒå¼±çš„å¤šå¤„ç†å™¨ç³»ç»Ÿï¼›
+ NUMAç³»ç»Ÿé€‚åˆè§„æ¨¡æ›´å¤§çš„å¤šå¤„ç†å™¨ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯éœ€è¦é«˜æ€§èƒ½å’Œå¯ ä¼¸ç¼©æ€§çš„åº”ç”¨ç¨‹åºï¼› 
+ MPPç³»ç»Ÿé€‚åˆå¤„ç†å¤§è§„æ¨¡æ•°æ®é›†å’Œéœ€è¦å¤§é‡è®¡ç®—èµ„æºçš„åº”ç”¨ç¨‹åº
#### Massively Parallel Processors (MPP)
100+ nodes with a high-speed interconnection network/switch; Each node has 1+ processors, sharing of the main memory;  Separate copy of OS runs on each node  
These MPP databases([BigQuery Enterprise Data Warehouse Â |Â  Google Cloud](https://cloud.google.com/bigquery/?utm_source=google&utm_medium=cpc&utm_campaign=japac-AU-all-en-dr-BKWS-all-rmkt-trial-EXA-dr-1605216&utm_content=text-ad-none-none-DEV_c-CRE_648938225949-ADGP_Hybrid+%7C+BKWS+-+EXA+%7C+Txt+~+Data+Analytics_BigQuery_big+query_main-KWID_43700075272952174-kwd-63326440124&userloc_9071445-network_g&utm_term=KW_google%20bigquery&gclid=Cj0KCQiA6rCgBhDVARIsAK1kGPKA3oXdZG1wbH_J35kmtVKdu9pO4Jk_3uhmNYYiC39FvMBaQEQhJ0caAmGnEALw_wcB&gclsrc=aw.ds)) are designed to handle large volumes of data and provide high  performance and scalability. They are commonly used in data warehousing and business intelligence application
#### Symmetric Multi-processors (SMP)
All global resources are shared among processors;  Nodes run the same OS (Enterprise servers; High-performance computing cluster; )   
SMP architecture is commonly used in medium- to large-scale multiprocessor systems, and  provides a simple and efficient way to achieve multiprocessing without the complexities of  NUMA (Non-Uniform Memory Access) or MPP (Massively Parallel Processing) architectures.  SMP systems are typically used for applications that require shared access to a moderate  amount of memory and do not require high scalability or perform


#### Non-Uniform Memory Access (NUMA)
Each processor has a global view of the available memory shared- address-space computer with local and global  memories (supercomputer; High-performance computing cluster)  
NUMA systems are typically used for large-scale parallel processing applications that require  high performance and scalability. They provide fast access to memory for each processor by  dividing the system memory into local and remote memory regions, and optimizing access to  these regions based on the location of the data being accessed. NUMA architecture can be  more complex and expensive to implement than UMA architecture, but it can provide better  performance and scalability for certain types of application
![](../img/fit5225-20230312-45.png)
![](../img/fit5225-20230312-46.png)
#### Clusters
Collection of workstations interconnected by a high-speed network, with nodes running a single system image

#### Distributed Systems
Grids/P2P/Internet Computing/Clouds
### Amdahlâ€™s Law
$1/((1-p)+(p/n))$  
![](../img/fit5225-20230312-32.png)
### Parallel Programming
Shared Memory  â€¢ Threads  â€¢ Message Passing  â€¢ Data Parallel  â€¢ Hybrid  
Why?  
- To save time  
- Solve bigger and more complex problems  
- Leverage concurrency  
- Itâ€™s very expensive to build high capacity and powerful CPUs and connect them together 

How  
- Phase parallel  
- Divide and conquer  
- Pipeline  
- Process farm  
- Work pool  
Limitations and Challenges  
+ Finding efficient ways of parallelizing the source code
+ Designing and developing scalable algorithms is challenging
+ Message passing and job submission are not automated and need to be taken care of by the  user/programmer
+ Managing jobs in a parallel and distributed environment is prone to lots of errors, think about  communication, resource allocation, etc
### Cluster Computing
Cluster computing is a collection of tightly or loosely connected computers that work together so that they act as a single entity. The connected computers execute operations all together thus creating the idea of a single system. <mark style="background: #0000CD;">The clusters are generally connected through fast local area networks (LANs)</mark>
### Grid Computing
grid as a system that coordinates resources which are not subject to centralized control,  using standard, open, general-purpose protocols and interfaces to deliver nontrivial qualities of service
### Summary of Cluster, Grid, and Cloud Computing
![](../img/fit5225-20230312-47.png)
### Open Multi-Processing (OpenMPI)
A master thread forks a number of sub-threads and divides tasks between them    
Open Multi-Processing (OpenMP) is an API that includes directives for multi-threaded, shared memory parallel programming. Thread-based rather than using message passing
### Message Passing Interface (MPI)
![](../img/fit5225-20230313.png)
MPI is a standardized and portable message-passing system to function on a wide variety of  parallel computers.  Widely adopted for inter-process communication in parallel systems. â€¢ Supports both p2p and collective modes of communication

## QUIZ
![](../img/fit5225-20230312.png)
![](../img/fit5225-20230312-1.png)
![](../img/fit5225-20230312-27.png)
![](../img/fit5225-20230312-28.png)

![](../img/fit5225-20230312-29.png)
![](../img/fit5225-20230312-30.png)
![](../img/fit5225-20230312-31.png)
![](../img/fit5225-20230312-2.png)
![](../img/fit5225-20230312-4.png)
![](../img/fit5225-20230312-5.png)
![](../img/fit5225-20230313-1.png)


## Tutorial
The main purpose of this tutorial is to help you gain and develop a better understanding of distributed systems, parallel programming, and message passing between processes. You will build, run, and debug multi-threaded programs as well as simple programs that use an MPI emulator to demonstrate message passing between processes running on different CPU cores.æœ¬æ•™ç¨‹çš„ä¸»è¦ç›®çš„æ˜¯å¸®åŠ©æ‚¨æ›´å¥½åœ°ç†è§£åˆ†å¸ƒå¼ç³»ç»Ÿã€å¹¶è¡Œç¼–ç¨‹å’Œè¿›ç¨‹é—´çš„æ¶ˆæ¯ä¼ é€’ã€‚æ‚¨å°†æ„å»ºã€è¿è¡Œå’Œè°ƒè¯•å¤šçº¿ç¨‹ç¨‹åºä»¥åŠä½¿ç”¨ MPI æ¨¡æ‹Ÿå™¨çš„ç®€å•ç¨‹åºï¼Œä»¥æ¼”ç¤ºåœ¨ä¸åŒ CPU å†…æ ¸ä¸Šè¿è¡Œçš„è¿›ç¨‹ä¹‹é—´çš„æ¶ˆæ¯ä¼ é€’ã€‚

# Week 3 Virtualization
## Lecture
+ Hardware Virtualization  
+ Virtualization Features: Isolation, Encapsulation, Portability, and Interpositionâ€™  
+ Hypervisor, Virtual Machine, Guest Operating System, Host  
+ Processor, Memory and I/O Devices virtualization  
+ Live and cold VM migration  
+ Case studies: Xen and VMWare  

describe fundamental principles and paradigms of cloud computing;  
demonstrate a comprehensive understanding of virtualisation and container technologies;  

### Virtualization
Virtualization is a broad concept that refers to the creation of a virtual version of something, whether  hardware, a software environment, storage, or a network.  

#### Motivations for virtualization
+ Optimal allocation of the virtual machines to physical server  
+ Virtualization is very relevant to the provision of cloud computing (multi-tenancy)  
+ Create and destroy virtual machines readily and with little overhead  
+ Several different operating system environments on a single desktop computer  
#### Virtualization Features
+ Isolationéš”ç¦» (Fault isolation, performance, software isolation)  
+ Encapsulationå°è£… (Cleanly capture all VM state, enables VM snapshots, clone VMs easily, make copies)  
+ Portabilityå¯ç§»æ¤æ€§ (Independent of physical hardware, enables live and cold migration of VMs)  
+ Interpositionæ’å…¥(Transformations on instructions, memory, I/O Enables transparent resource over commitment, encryption, compression, replication)
#### Virtualization Types
![](../img/fit5225-20230313-4.png)
+ Full virtualization : identical interface to the underlying physical architecture  unmodified guest operating system(VMware, Virtualbox)  
+ Paravirtualization : guest operating systems to communicate with the hypervisor.  guest os to be modified in order to interact with paravirtualization interface  communicates with the hypervisor using driver  higher performance (XEN)

### Hardware Virtualization  
The term virtualization is often synonymous with hardware virtualization/system virtualization.  â€œa technique for hiding the physical characteristics of computing resources which allows having  multiple virtual machines (VMs) over the underlying physical machine architecture, with each virtual  machine running a separate operating system instance.
### Hypervisor ç®¡ç†ç¨‹åº
![](../img/fit5225-20230313-2.png)
+ Hardware virtualization is implemented by a thin layer of software on  top of the underlying physical machine architecture referred to as  Hypervisor (Virtual Machine Monitor).  
+ Virtual Machine: A representation of a real machine using  hardware/software that can host a guest operating system  
+ Guest Operating System: An operating system that runs in a virtual  machine  
+ Host: the original environment where the guest OS is supposed to  be managed

### Computing Systems
![](../img/fit5225-20230313-3.png)
1. Instruction Set Architecture (ISA)  
2. Application Binary Interface (ABI)  
3. Application programming interface (API)  
For any operation to be performed in the  application level API, ABI and ISA are  responsible for making it happen

### Virtualizing Memory

## Tutorial
upon successful completion of this tutorial, you will gain the required knowledge to access Oracle Cloud Infrastructure  (OCI), create/modify virtual machines, running a micro web framework, and working with Postman  
1. `sudo apt-get update && sudo apt-get install -y python3-pip curl && pip3 install flask` install the `python3-pip` and `curl` packages, and then install the `flask` package using `pip3`  
2. `touch flask_script.py` create py file  
3. `nano flask_script.py` open the file in the nano text editor 
4. `sudo apt update` check for any updates available for the installed packages and update  
5. `sudo apt install firewalld` install firewall service that provides a dynamically managed firewall with support for network/firewall zones
6. `sudo systemctl enable firewalld` `firewalld` service will start automatically every time the system boots up  
7. `sudo firewall-cmd --permanent --zone=public --add-port=5000/tc` add a permanent rule to the firewall on Ubuntu to allow incoming traffic on port 5000 for TCP protocol.  
8. `sudo firewall-cmd --reload` reload the `firewalld` firewall configuration. This means that any changes made to the firewall rules, such as adding or removing ports, will take effect immediately without having to restart the firewall service.  
9. `python3 flask_script.py &`  command that runs the `flask_script.py` Python script in the background as a separate process  
10. `curl -iX GET <IP-ADDR>:5000/` use the `curl` tool to make an HTTP GET request to the web server running on the specified IP address and port 5000. The server will then respond with an HTTP response that includes the requested data, and the `-i` option will cause `curl` to display the HTTP header along with the response body.

# Week4  Containers
describe fundamental principles and paradigms of cloud computing;  
demonstrate a comprehensive understanding of virtualisation and container technologies
## Lecture
VMs and Containers  
Docker as a containers enabler (Docker Engine Components  Docker Images, Image registries, Layers, Dockerfile )
Container Orchestration (DockerSwarm  Kubernetes ) 
### Virtualization Advantages and Problems
Advantages:  
+ Minimize hardware costs (CapEx)  
+ Easily move VMs to other data centre  
+ Consolidate idle workloads  
+ Easier automation (Lower OpEx)

Problems:  
+ Every VM requires its own dedicated OS  
+ Needs maintenance, update  
+ VMs are slow to boot  Portability issues Between hypervisors and cloud platforms  
### Containers
+ Containers on a single host share a single OS.  
+ Containers are fast to start  
+ Containers are ultra-portable (own file system/data, own networking)  
+ Containers are isolated
+ Multiple copies can be run on the same machine or different machine â‡’ Scalable  Same image can run on a personal machine, in a data centre or in a cloud
+ Can be stopped, saved and moved to another machine or for later run  
### Containers vs. VMs
[Whatâ€™s the Diff: VMs vs. Containers](https://www.backblaze.com/blog/vm-vs-containers/)
![](../img/fit5225-20230320-1.png)  

|                              | VMs                                       | Containers                                    |
| ---------------------------- | ----------------------------------------- | --------------------------------------------- |
| Type                         | Heavyweight                               | Lightweight                                   |
| Performance                  | Limited performance                       | Native performance                            |
| CPU overhead                 | >10%                                      | <5%                                           |
| Disk I/O                     | >50%                                      | Negligible                                    |
| OS                           | Each VM runs in its own OS                | All containers share the host OS              |
| Virtualization               | Hardware-level virtualization             | OS virtualization                             |
| Boot time                    | In minutes                                | In seconds/milliseconds                       |
| Memory                       | Allocates required memory                 | Requires less memory space                    |
| Security                     | Fully isolated and hence more secure (hypercalls)      | Process-level isolation, possibly less secure (syscalls on the shared kernel)|
| Impact on Legacy Application | Low-medium                                | High impact on legacy application             |
| applications                 | Multiple applications on multiple servers | Many copies of a single application           |
|                              |                                           |                                               |

### Docker
+ made Linux containers usable for everybody.
+ Most of the project and its tools are written in Golang
### DevOps  
- Ops Perspective  
	- download an image,  
	- start a new container,  
	- log in to the new container,
	- run a command inside of it, and then destroy it.  

- Dev Perspective  
	- pull some app-code from GitHub,  
	- inspect a Dockerfile,  
	- containerize the app,  
	- run it as a container.  

### Docker Engine
![](../img/fit5225-20230320-2.png)  
+ Docker Daemon manages containers, images, builds and  more  
+ Docker Client communicates with the Docker Daemon to  execute commands  
+ A REST API can be used for interacting with the Docker  Daemon remotely  
+ Docker Images are read-only templates that you build from a  set of instructions written in your Dockerfile  
+ Docker container wraps an applicationâ€™s software into a box  with everything the application needs to run
+ Docker registry is a storage and distribution system for  named Docker images.  The registry allows Docker users to pull images locally, as  well as push new images to the registry (given adequate  access permissions when applicable).

![](../img/fit5225-20230320-4.png)  
Docker engine main components:  
- Docker client  
- Docker daemon  
	- Image management, image builds, the  REST API, authentication, security, core  networking, and orchestration  
- containerd  
	- container lifecycle management  
	- start | stop | pause | rm....  
- runc  
- a small, lightweight CLI wrapper for  libcontainer

### Images
Containers are built from images and can be saved as image  The process of getting images onto a Docker host is called pulling `$ docker image pull ubuntu:latest`  ubuntu is the repository and latest is the tag
### layers
![](../img/fit5225-20230321.png)  

+ Docker image is just a bunch of loosely- connected read-only layer  
+ image is built layer by layer  
+ Layers in an image can be inspected by Docker  commands  
+ A file in the higher layer obscures the file directly  below it.  This allows updated versions of files to be added as  new layers to the image  
+ Multiple images can, and do, share layers. This leads  to efficiencies in space and performance
### Starting a new container  
`$ docker container run --name  ctr1 -it alpine:latest sh` This command creates and runs a new container named ctr1 from the alpine:latest image and attaches an interactive shell to it. 
![](../img/fit5225-20230321-1.png)  
1. When you type command into the Docker CLI, the Docker  client converts them into the appropriate API payload and  POSTs them to the correct API endpoint  
2. The API is implemented in the daemon  
3. Once the daemon receives the command to create a new  container, it makes a call to containerd   
4. containerd uses runc to create containe  
5. runc interfaces with the OS kernel to pull together all of the  constructs necessary to create a container (namespaces,  cgroups etc
### Docker command
- `$ docker container ls`: This command lists all the running containers on the system. It shows information such as container ID, image name, command, creation time, status, and port mappings.
- `$ docker container exec -it 3027eb644874 bash`: This command executes a bash shell in an existing container with ID 3027eb644874. The -it option makes the shell interactive and attaches it to the terminal.
- `$ docker container stop 3027eb64487`: This command stops a running container with ID 3027eb64487. It sends a SIGTERM signal to the main process inside the container and waits for a grace period before killing it with SIGKILL if it does not exit.
- `$ docker container rm 3027eb64487`: This command removes a stopped container with ID 3027eb64487. It deletes all the files and data associated with the container from the system.

## Tutorial
### Install Docker
1. `sudo apt update`Â updates the package index and the software repositories on the system. It retrieves the information about the new or updated packages versions, dependencies, and their URLs.  
2. `sudo apt install -y apt-transport-https ca-certificates curl gnupg software-properties-common lsb-release` this command is installing essential packages required for managing and installing software  
3. `sudo mkdir -m 0755 -p /etc/apt/keyrings` Â this command creates a new directory namedÂ `keyrings`Â under theÂ `/etc/apt`Â directory with specified permissions. The purpose of this directory is to store GPG keys that are required to authenticate packages in apt repositories. 
4. `curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg`  downloads the Docker GPG key file, decrypts it usingÂ `gpg`, and saves it in theÂ `/etc/apt/keyrings`Â directory with the nameÂ `docker.gpg`. This allows theÂ `apt`Â package manager to authenticate and verify packages from Docker repositories.  
5. `sudo chmod a+r /etc/apt/keyrings/docker.gpg` this command allows all users to read the Docker GPG key file (`docker.gpg`). This is necessary to allow theÂ `apt`Â package manager to authenticate and verify packages from Docker repositories.  
6. `echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null` this command adds the Docker repository information to theÂ `docker.list`Â file in theÂ `/etc/apt/sources.list.d`Â directory. This file is used by theÂ `apt`Â package manager to download and install packages from the Docker repository using the GPG key that was installed earlier.  
7. `sudo apt update`  
8. `sudo apt install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin` this command installs the Docker platform and related components using theÂ `apt`Â package manager. After installation, theÂ `docker`Â command should be available in the terminal, and Docker should be ready to use on the system  
9.  `sudo usermod -aG docker ${USER}`  this command adds the current user to theÂ `docker`Â group, enabling them to use Docker without needing to enterÂ `sudo`Â before every command. However, you may need to log out of the current shell or open a new terminal window for the group changes to take effect.
### Managing Containers
1. theÂ `docker run hello-world`Â command downloads theÂ `hello-world`Â image from the Docker Hub repository and creates a new container using that image. The container then prints a message on the command-line interface that confirms the successful installation of Docker on the system  
2.  `docker ps -a` command lists all the containers on the system, including those that are currently running and those that have stopped. It provides information such as the container's ID, image name, status, creation time, and exit code.  
3. `docker image list` s all the Docker images that are present on the system. It provides information such as the image's repository name, tag, image ID, and creation time.  
4. `docker pull ubuntu`  downloads the latest version of the Ubuntu image from the Docker Hub repository and saves it locally on the system  
5. `docker run ubuntu sh -c "echo Hello FIT5225 from ubuntu; ls; echo bye guys"` creates a container from theÂ `ubuntu`Â image and runs a command to print "Hello FIT5225 from ubuntu", list the contents of the current directory, and print "bye guys".  
6. `docker run -it ubuntu bash` command creates a new container from theÂ `ubuntu`Â image and starts a new interactive session with a Bash shell or command prompt inside the container By default, the container will not be deleted after exit and if  you run `sudo docker ps -a` again you will see a list of existing containers.  
7. `docker run -it --rm ubuntu bash` `--rm`Â is an option to theÂ `run`Â command that tells Docker to automatically remove the container when it exits. This helps keep the disk space of your machine clean by automatically cleaning up unnecessary containers. When executed, theÂ `docker run -it --rm ubuntu bash`Â command creates a new container from theÂ `ubuntu`Â image, starts a new interactive session with a Bash shell or command prompt inside the container, and automatically removes the container when it exits.  
8. `docker run -t -d ubuntu`Â command creates a new container from theÂ `ubuntu`Â image, starts the container in the background or in detached mode, and returns the container ID to the console.  
9. `docker exec -i -t <container-id> bash` To create an interactive shell to a running container 
10. `docker stop <container-id>` To stop the container  
11. `docker rm <container-id>` to And to remove the container  
12. `docker rm -f $(docker ps -a -q)` And to remove all containers

### Running a web Server 
1. `docker run --name myserver -p 8080:80 -d nginx`  
2. Create a sample html file in a local folder under your home directory named â€œwwwâ€, name it as index.html 
3. Change the current directory to your home directory by running the following command: `cd ~` Create a new directory named "www" in your home directory by running the following command: `mkdir www`  
4. Create a new file named "index.html" in the "www" directory and open it with the Nano editor by running the following command:`nano index.html`  Copy and paste the following content into the Nano editor Save the file and exit the Nano editor by pressingÂ `Ctrl-X`, thenÂ `Y`, thenÂ `Enter`
5. `curl <ip address of your machine>:8080` execute a GET request to the specified IP address of your machine on port 8080 using theÂ `curl`Â utility  Open port 8080 in your security group, verify that you can access the web page using your browser 
### Creating Docker Images  
 `mkdir demo_<YourName>`  Create a requirements.txt file `flask`;   
 Create a file named my_script.py;  
 ```py
 from flask import Flask  
app = Flask(__name__)  
@app.route("/")  
def hello():  
	return "Hello World from FIT5225!"  
if __name__ == "__main__":  
	app.run(host='0.0.0.0'
```  
 Create a file named Dockerfile:  
 ```
 FROM python:3.7-alpine  
WORKDIR /code  
ADD my_script.py /code  
COPY requirements.txt requirements.txt  
RUN pip install -r requirements.txt  
CMD ["python", "/code/my_script.py"]
```
Dockerfile that starts with a python 3.7 image based on Alpine Linux, sets the working directory toÂ `/code`, addsÂ `my_script.py`Â to the working directory, copies theÂ `requirements.txt`Â file to the working directory, installs the requirements using pip, and finally runs the commandÂ `python /code/my_script.py`Â when the container starts.  
`docker build -t flask-app .` build an image from the Dockerfile  
`docker run -d -p 5000:5000 --name my-flask-app flask-app` Â starts a Docker container in detached mode (`-d`), maps portÂ `5000`Â inside the container to portÂ `5000`Â on the host machine (`-p 5000:5000`), names the containerÂ `my-flask-app`Â (`--name my-flask-app`), and uses theÂ `flask-app`Â image to start the container.  

# Week 5 Container Orchestration
.Container orchestration is the automation of the operational effort required to run containerized workloads and services. This includes a wide range of things software teams need to manage a container's lifecycle, including provisioning, deployment, scaling (up and down), networking, load balancing and more. This week, we will introduceÂ DockerSwarm and Kubernetes as two well-known container orchestration methods when we run a cluster of containers.  
-   describe fundamental principles and paradigms of cloud computing
-   demonstrate a comprehensive understanding of virtualisation and container technologies
## Lecture

## Tutorial
1. we need to create 3 VMs in Oracle cloud, each with 2 cores (AMD) and 8GB RAM, use Ubuntu 22.04 image and  tick the Burstable option with 50% baseline. This option will give you 50% cost saving, but itâ€™s able to burst to higher  CPU frequency when needed.  Name the 3 VMs as: k8s-master, k8s-worker1, k8s-worker2  You need to open TCP port 6443 on your controller(master) and worker nodes.
2. Use ssh to connect to all 3 VMs and follow these steps `sudo apt update && sudo apt install -y apt-transport-https ca-certificates curl gnupg software-properties-common lsb-release && sudo mkdir -m 0755 -p /etc/apt/keyrings && curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg && sudo chmod a+r /etc/apt/keyrings/docker.gpg && echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null && sudo apt update && sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin && sudo usermod -aG docker ${USER}` Then logout and login again  
3. Use ssh to connect to all 3 VMs and follow these steps: `wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.1/cri-dockerd_0.3.1.3-0.ubuntu-jammy_amd64.deb && sudo dpkg -i cri-dockerd_0.3.1.3-0.ubuntu-jammy_amd64.deb && sudo apt update && sudo apt-get install -y apt-transport-https ca-certificates curl && sudo curl -fsSLo /etc/apt/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg && echo "deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list && sudo apt update && sudo apt-get install -y kubelet kubeadm kubectl && sudo apt-mark hold kubelet kubeadm kubectl && kubeadm version` 
4. Open ports with iptables(Oracle Cloud)  Oracle-provided images have strict traffic control for security reasons. In addition to the VCN security list, you also need  to edit the iptables for all nodes `sudo nano /etc/iptables/rules.v4` 
5. initialise Kubeadm: (master only) `sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --cri-socket=unix:///var/run/cri-dockerd.sock --apiserver-advertise-address=<master private ip>` and it will print such result `Then you can join any number of worker nodes by running the following on each as root: kubeadm join *** --token **`   
6. After initialisation, on master node only ` mkdir -p $HOME/.kube && sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config && sudo chown $(id -u):$(id -g) $HOME/.kube/config`  
7. Now you can join your worker nodes with the provided join command, then check if the nodes are ready. Please note the command has to be executed with sudo and specify which cri-socket to connect `sudo <JOIN_COMMAND> --cri-socket=unix:///var/run/cri-dockerd.sock` (master node) `kubectl get nodes`![](../img/fit5225-20230415.png)  
8. 

# Quiz 2
[Quiz2 exercise 1](https://shareg.pt/DzLeCyM)  
[Quiz 2 exercise 2](https://shareg.pt/QECNMS5)  
[Quiz2 moodle](https://shareg.pt/lpYEJND)  

# Week 6 Web Service and Service-Oriented Architecture
[week6 flux quiz](https://shareg.pt/sPy7cn0)
[Flask REST API Tutorial - Python Tutorial](https://pythonbasics.org/flask-rest-api/)  
[Flask-RESTful â€” Flask-RESTful 0.3.8 documentation](https://flask-restful.readthedocs.io/en/latest/)  
This week in the first part lecture, we will learn the fundamentals of web services and best practices to create and use them includingÂ SOAPÂ andÂ REST.Â AÂ web serviceÂ is a basic building block in service-oriented architecture (SOA).Â SOA is an architectural pattern in computer software design in which application components provide services to other components via a communications protocol, typically over a network. The principles of service orientation are independent of any product, vendor or technology.Â   
A Web service is a software system designed to support interoperable machine-to-machine interaction over the network. It has an interface defined in a specific description and format. Other machines interact with the Web service in a manner prescribed by its description, typically conveyed using HTTP with an XML or JSON serialization in conjunction with other Web-related standards.   
## Learning Objectives
-   describe fundamental principles and paradigms of cloud computing;
-   identify appropriate design choices when developing real-world cloud computing applications;Â 
-   apply different cloud programming methods and tools;

# Assignment 1
[assignment 1](fit5225_note.md)  
[git repository for FIT5225_23s1/assignment1/](https://github.com/GreenH47/FIT5225_23s1/tree/main/assignment1)  

# Week 7 Infrastructure Automation and Code Versioning
Infrastructure automation refers to the use of software tools and processes to automatically provision, configure, manage, and monitor IT infrastructure resources. This approach reduces the manual effort and errors associated with managing infrastructure, improves agility, scalability, and reliability, and enables organisations to focus on delivering value to their customers. Infrastructure automation is widely used in cloud computing. Thus, this week, we spend some time discussing Infrastructure Automation and Infrastructure as code (IaC). Code versioning, which is often integrated into infrastructure automation workflows, allows developers to track and manage changes to code and infrastructure configurations over time, enabling better collaboration, code reuse, and auditing. We also cover code versioning and Git this week.
åŸºç¡€è®¾æ–½è‡ªåŠ¨åŒ–æ˜¯æŒ‡ä½¿ç”¨è½¯ä»¶å·¥å…·å’Œæµç¨‹è‡ªåŠ¨ä¾›åº”ã€é…ç½®ã€ç®¡ç†å’Œç›‘æ§ IT åŸºç¡€è®¾æ–½èµ„æºã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†ä¸ç®¡ç†åŸºç¡€æ¶æ„ç›¸å…³çš„æ‰‹åŠ¨å·¥ä½œå’Œé”™è¯¯ï¼Œæé«˜äº†æ•æ·æ€§ã€å¯æ‰©å±•æ€§å’Œå¯é æ€§ï¼Œå¹¶ä½¿ç»„ç»‡èƒ½å¤Ÿä¸“æ³¨äºä¸ºå®¢æˆ·æä¾›ä»·å€¼ã€‚åŸºç¡€è®¾æ–½è‡ªåŠ¨åŒ–åœ¨äº‘è®¡ç®—ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚å› æ­¤ï¼Œæœ¬å‘¨ï¼Œæˆ‘ä»¬å°†èŠ±ä¸€äº›æ—¶é—´è®¨è®ºåŸºç¡€è®¾æ–½è‡ªåŠ¨åŒ–å’ŒåŸºç¡€è®¾æ–½å³ä»£ç  (IaC)ã€‚ä»£ç ç‰ˆæœ¬æ§åˆ¶é€šå¸¸é›†æˆåˆ°åŸºç¡€æ¶æ„è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ä¸­ï¼Œå®ƒå…è®¸å¼€å‘äººå‘˜éšæ—¶é—´è·Ÿè¸ªå’Œç®¡ç†ä»£ç å’ŒåŸºç¡€æ¶æ„é…ç½®çš„æ›´æ”¹ï¼Œä»è€Œå®ç°æ›´å¥½çš„åä½œã€ä»£ç é‡ç”¨å’Œå®¡è®¡ã€‚æœ¬å‘¨æˆ‘ä»¬è¿˜ä»‹ç»äº†ä»£ç ç‰ˆæœ¬æ§åˆ¶å’Œ Gitã€‚

Cloud Platforms: AWS (Part 1) äº‘å¹³å°ï¼šAWSï¼ˆç¬¬ 1 éƒ¨åˆ†ï¼‰
We will continue our discussion on cloud platforms and introduce Amazon Web Services (AWS). AWS is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments on a metered, pay-as-you-go basis. AWS offers wide range of products and cloud services, including computing, storage, networking, databases, analytics, application services, deployment, management, mobile, developer tools, and IoT services. This lecture will provide a brief overview of these services.
æˆ‘ä»¬å°†ç»§ç»­è®¨è®ºäº‘å¹³å°å¹¶ä»‹ç» Amazon Web Services (AWS)ã€‚ AWS æ˜¯ Amazon çš„å­å…¬å¸ï¼Œä»¥æŒ‰éœ€ä»˜è´¹çš„æ–¹å¼å‘ä¸ªäººã€å…¬å¸å’Œæ”¿åºœæä¾›æŒ‰éœ€äº‘è®¡ç®—å¹³å°å’Œ APIã€‚ AWS æä¾›å¹¿æ³›çš„äº§å“å’Œäº‘æœåŠ¡ï¼ŒåŒ…æ‹¬è®¡ç®—ã€å­˜å‚¨ã€ç½‘ç»œã€æ•°æ®åº“ã€åˆ†æã€åº”ç”¨ç¨‹åºæœåŠ¡ã€éƒ¨ç½²ã€ç®¡ç†ã€ç§»åŠ¨ã€å¼€å‘äººå‘˜å·¥å…·å’Œç‰©è”ç½‘æœåŠ¡ã€‚æœ¬è®²åº§å°†ç®€è¦æ¦‚è¿°è¿™äº›æœåŠ¡ã€‚

Learning Objectives å­¦ä¹ ç›®æ ‡
Understand the benefits of infrastructure automation and learn about Infrastructure as Code (IaC) such as Ansible.
äº†è§£åŸºç¡€è®¾æ–½è‡ªåŠ¨åŒ–çš„å¥½å¤„å¹¶äº†è§£åŸºç¡€è®¾æ–½å³ä»£ç  (IaC)ï¼Œä¾‹å¦‚ Ansibleã€‚
Explore the use of code versioning tools like Git for tracking changes to code.
æ¢ç´¢ä½¿ç”¨ Git ç­‰ä»£ç ç‰ˆæœ¬æ§åˆ¶å·¥å…·æ¥è·Ÿè¸ªä»£ç æ›´æ”¹ã€‚
Gain a basic understanding of Amazon Web Services (AWS) and the wide range of products and cloud services it offers.
åŸºæœ¬äº†è§£äºšé©¬é€Šç½‘ç»œæœåŠ¡ (AWS) åŠå…¶æä¾›çš„å„ç§äº§å“å’Œäº‘æœåŠ¡ã€‚
# lecture
[ğŸŒ³ğŸš€ CS Visualized: Useful Git Commands - DEV Community](https://dev.to/lydiahallie/cs-visualized-useful-git-commands-37p1)  

## Tutorial 
This tutorial covers basics of working with Git repositories and common tasks in code versioning systems. You will learn how a Git repository is initialised and get to know the commands that are used for creating a new branch, staging changes and committing them, and performing merge and rebase operations. Additionally, you will learn concepts around configuration automation and how tools such as Ansible can be used to remotely configure multiple servers simultaneously. Finally, you will use Ansible to automate creation of various resources in Nectar and create a small Kubernetes cluster.Â Â Â Â   
æœ¬æ•™ç¨‹æ¶µç›–äº†ä½¿ç”¨ Git å­˜å‚¨åº“çš„åŸºç¡€çŸ¥è¯†ä»¥åŠä»£ç ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿä¸­çš„å¸¸è§ä»»åŠ¡ã€‚æ‚¨å°†äº†è§£ Git å­˜å‚¨åº“æ˜¯å¦‚ä½•åˆå§‹åŒ–çš„ï¼Œå¹¶äº†è§£ç”¨äºåˆ›å»ºæ–°åˆ†æ”¯ã€æš‚å­˜æ›´æ”¹å’Œæäº¤æ›´æ”¹ä»¥åŠæ‰§è¡Œåˆå¹¶å’Œå˜åŸºæ“ä½œçš„å‘½ä»¤ã€‚æ­¤å¤–ï¼Œæ‚¨è¿˜å°†äº†è§£æœ‰å…³é…ç½®è‡ªåŠ¨åŒ–çš„æ¦‚å¿µï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ Ansible ç­‰å·¥å…·åŒæ—¶è¿œç¨‹é…ç½®å¤šä¸ªæœåŠ¡å™¨ã€‚æœ€åï¼Œæ‚¨å°†ä½¿ç”¨ Ansible åœ¨ Nectar ä¸­è‡ªåŠ¨åˆ›å»ºå„ç§èµ„æºå¹¶åˆ›å»ºä¸€ä¸ªå°å‹ Kubernetes é›†ç¾¤ã€‚

1. `sudo apt update && sudo apt install -y software-properties-common git curl python3 && sudo apt install -y python3-pip && export PATH=$PATH:/home/ubuntu/.local/bin`  
2. `mkdir -p ~/git-demo && echo â€œ# Git is coolâ€ > ~/git-demo/README.md && cd ~/git-demo` 


# week 8 Compute and Storage Services in AWS
## Tutorial
### Goals
The purpose of this tutorial is to familiarise you with the basic concepts around compute and storage services in AWS. You will create virtual machines using the EC2 service and experiment multiple ways of remotely connecting to your VMs. Additionally, you will explore AWS S3 that provides a secure and efficient object storage service and learn how to create and access a fully managed relational database using AWS RDS.  
### Terminology  
1. (Elastic Compute Cloud)EC2: An Amazon web service that provides scalable computing in the cloud with a pricing policy that you pay as you go, pay for what you use and pay less when you use more or reserve capacity.  
2. (Simple Storage Service)S3: Amazon Simple Storage Service provides scalable and reliable storage and retrieval of objects through a web service interface.  
3. EBS: Amazon Elastic Block Store provides an easy to manage block level persistent volumes to be used with EC2 instances
### How to do it

# Quiz 3
[Check out this ShareGPT conversation](https://shareg.pt/O3QfEiw) 

# Week 9 Serverless and Function-as-a-Service (FaaS)
This week we will wrap up the remaining services of AWS and cover VPC, SQS, etc. Then we will begin our discussion on Serverless computing which revolutionized how cloud computing is done. Serverless Computing is a cloud computing execution model in which the cloud provider runs the server, and dynamically manages the allocation of machine resources. Pricing is based on the actual amount of resources consumed by an application, rather than on pre-purchased units of capacity. This week, the serverless concept is introduced first, and then, as a use case, we discuss AWS serverless service calledÂ _Lambda_. In the lecture, I'll show you a demo of how Lambda can be used and in your tutorial, you will design your first lambda functions.  
æœ¬å‘¨æˆ‘ä»¬å°†æ€»ç»“ AWS çš„å‰©ä½™æœåŠ¡å¹¶æ¶µç›– VPCã€SQS ç­‰ã€‚ç„¶åæˆ‘ä»¬å°†å¼€å§‹è®¨è®ºæ— æœåŠ¡å™¨è®¡ç®—ï¼Œå®ƒå½»åº•æ”¹å˜äº†äº‘è®¡ç®—çš„å®Œæˆæ–¹å¼ã€‚æ— æœåŠ¡å™¨è®¡ç®—æ˜¯ä¸€ç§äº‘è®¡ç®—æ‰§è¡Œæ¨¡å‹ï¼Œå…¶ä¸­äº‘æä¾›å•†è¿è¡ŒæœåŠ¡å™¨ï¼Œå¹¶åŠ¨æ€ç®¡ç†æœºå™¨èµ„æºçš„åˆ†é…ã€‚å®šä»·åŸºäºåº”ç”¨ç¨‹åºæ¶ˆè€—çš„å®é™…èµ„æºé‡ï¼Œè€Œä¸æ˜¯åŸºäºé¢„å…ˆè´­ä¹°çš„å®¹é‡å•ä½ã€‚æœ¬å‘¨ï¼Œé¦–å…ˆä»‹ç»æ— æœåŠ¡å™¨æ¦‚å¿µï¼Œç„¶åä½œä¸ºä¸€ä¸ªç”¨ä¾‹ï¼Œæˆ‘ä»¬è®¨è®ºç§°ä¸º Lambda çš„ AWS æ— æœåŠ¡å™¨æœåŠ¡ã€‚åœ¨è®²åº§ä¸­ï¼Œæˆ‘å°†å‘æ‚¨æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Lambdaï¼Œåœ¨æ‚¨çš„æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†è®¾è®¡æ‚¨çš„ç¬¬ä¸€ä¸ª lambda å‡½æ•°ã€‚

### Learning Objectives:Â å­¦ä¹ ç›®æ ‡ï¼š

-   describe fundamental principles and paradigms of cloud computing;  æè¿°äº‘è®¡ç®—çš„åŸºæœ¬åŸåˆ™å’ŒèŒƒä¾‹ï¼›
-   identify appropriate design choices when developing real-world cloud computing applications;  åœ¨å¼€å‘çœŸå®ä¸–ç•Œçš„äº‘è®¡ç®—åº”ç”¨ç¨‹åºæ—¶ç¡®å®šé€‚å½“çš„è®¾è®¡é€‰æ‹©ï¼›
-   apply different cloud programming methods and tools;  åº”ç”¨ä¸åŒçš„äº‘ç¼–ç¨‹æ–¹æ³•å’Œå·¥å…·ï¼›
# tutorial 
The purpose of this tutorial is to provide a hands-on experience with Serverless Architecture in AWS and demonstrate how business logic of a typical web application or even a complete API can be created using AWS Serverless technologies. You will learn the basics of working with AWS Lambda, including creating, testing, and deploying Lambda functions through the AWS console or via AWS SAM.Â Â Additionally, you will explore AWS API Gateway that provides a robust solution for implementing REST and WebSocket services and learn how to create and query a managed NoSQL database service using AWS DynamoDB.Â Â Â Â Â   
æœ¬æ•™ç¨‹çš„ç›®çš„æ˜¯æä¾› AWS ä¸­æ— æœåŠ¡å™¨æ¶æ„çš„å®è·µç»éªŒï¼Œå¹¶æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ AWS æ— æœåŠ¡å™¨æŠ€æœ¯åˆ›å»ºå…¸å‹ Web åº”ç”¨ç¨‹åºç”šè‡³å®Œæ•´ API çš„ä¸šåŠ¡é€»è¾‘ã€‚æ‚¨å°†å­¦ä¹ ä½¿ç”¨ AWS Lambda çš„åŸºç¡€çŸ¥è¯†ï¼ŒåŒ…æ‹¬é€šè¿‡ AWS æ§åˆ¶å°æˆ– AWS SAM åˆ›å»ºã€æµ‹è¯•å’Œéƒ¨ç½² Lambda å‡½æ•°ã€‚æ­¤å¤–ï¼Œæ‚¨è¿˜å°†æ¢ç´¢ AWS API Gatewayï¼Œå®ƒä¸ºå®æ–½ REST å’Œ WebSocket æœåŠ¡æä¾›äº†å¼ºå¤§çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å­¦ä¹ å¦‚ä½•ä½¿ç”¨ AWS DynamoDB åˆ›å»ºå’ŒæŸ¥è¯¢æ‰˜ç®¡ NoSQL æ•°æ®åº“æœåŠ¡ã€‚
Function as a Service (FaaS): A category of cloud computing services that provides a platform allowing customers to  develop, run, and manage application functionalities without the complexity of building and maintaining the infrastructure typically associated with developing and launching an app.  
Lambda: AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time  you consume.  
API Gateway: A fully managed service that makes it easy for developers to create, publish, maintain, monitor, and  secure APIs at any scale. APIs act as the "front door" for applications to access data, business logic, or functionality from your backend services.  
DynamoDB: A fully managed key-value and document database that delivers single-digit millisecond performance at any scale with built-in security, backup and restore, and in-memory caching
## Implement Applicationâ€™s Business Logic Using Lambda
```js
const AWS = require('aws-sdk');
const dynamo = new AWS.DynamoDB.DocumentClient();

exports.handler = async (event, context, callback) => {
    console.log('remaining time =', context.getRemainingTimeInMillis());
    console.log('functionName =', context.functionName);
    console.log('AWSrequestID =', context.awsRequestId);

    let body;
    let statusCode = '200';
    const headers = { 'Content-Type': 'application/json' };
    const tableName = 'todos';

    try {
        switch (event.httpMethod) {
            case 'DELETE':
                body = await dynamo.delete(JSON.parse(event.body)).promise();
                break;
            case 'GET':
                if (event.pathParameters && event.pathParameters.taskId) {
                    let params = {
                        TableName: tableName,
                        FilterExpression: "id = :id",
                        ExpressionAttributeValues: { ":id": event.pathParameters.taskId },
                    };
                    body = await dynamo.scan(params).promise();
                } else {
                    body = await dynamo.scan({ TableName: tableName }).promise();
                }
                break;
            case 'POST':
                var data = {};
                data.desc = event.body && JSON.parse(event.body.trim()).desc || '';
                data.done = event.body && JSON.parse(event.body.trim()).done || false;
                data.title = event.body && JSON.parse(event.body.trim()).title || '';
                data.id = context.awsRequestId;
                data.updatedAt = new Date().getTime();

                let params = { TableName: tableName, Item: data };
                body = await dynamo.put(params).promise();
                body.message = data;
                statusCode = '201';
                break;
            case 'PUT':
                body = await dynamo.update(JSON.parse(event.body)).promise();
                break;
            default:
                throw new Error(`Unsupported method "${event.httpMethod}"`);
        }
    } catch (err) {
        statusCode = '400';
        body = err.message;
    } finally {
        body = JSON.stringify(body);
    }
    return { statusCode, body, headers };
};

```

use this to test your Lambda with post method
```json
{  
	"body": "{\n \"title\" : \"First Task\"\n,\"desc\" : \"sample description\"}",  
	"httpMethod": "POST"  
}
```
response
```json
test

Response
{
  "statusCode": "201",
  "body": "{\"message\":{\"desc\":\"sample description\",\"done\":false,\"title\":\"First Task\",\"id\":\"5578789a-c8fd-44b1-af8b-d2803dd6527a\",\"updatedAt\":1682928137779}}",
  "headers": {
    "Content-Type": "application/json"
  }
}

Function Logs
START RequestId: 5578789a-c8fd-44b1-af8b-d2803dd6527a Version: $LATEST
2023-05-01T08:02:17.778Z	5578789a-c8fd-44b1-af8b-d2803dd6527a	INFO	remaining time = 2990
2023-05-01T08:02:17.779Z	5578789a-c8fd-44b1-af8b-d2803dd6527a	INFO	functionName = todo_handler
2023-05-01T08:02:17.779Z	5578789a-c8fd-44b1-af8b-d2803dd6527a	INFO	AWSrequestID = 5578789a-c8fd-44b1-af8b-d2803dd6527a
END RequestId: 5578789a-c8fd-44b1-af8b-d2803dd6527a
REPORT RequestId: 5578789a-c8fd-44b1-af8b-d2803dd6527a	Duration: 930.71 ms	Billed Duration: 931 ms	Memory Size: 128 MB	Max Memory Used: 84 MB	Init Duration: 472.33 ms

Request ID
5578789a-c8fd-44b1-af8b-d2803dd6527a
```

modify the template to test requests with HTTP GET method with specific task ID.
```json
{
    "httpMethod": "GET",
    "pathParameters": {
        "taskId": "taskIdValue"
    }
}

```
Part of the aws lambda code that processes DELETE and PUT requests is not implemented correctly. Can you spot the issues and make required modifications to the code, so it works as expected  

For theÂ `DELETE`Â request:  å¯¹äºÂ `DELETE`Â è¯·æ±‚ï¼š
1.  UseÂ `JSON.parse(event.pathParameters.taskId)`Â instead ofÂ `JSON.parse(event.body)`. In the previous implementation, you were trying to parse theÂ `body`Â of the event, which should not contain any information for aÂ `DELETE`Â request.  ä½¿ç”¨Â `JSON.parse(event.pathParameters.taskId)`Â è€Œä¸æ˜¯Â `JSON.parse(event.body)`Â ã€‚åœ¨ä¹‹å‰çš„å®ç°ä¸­ï¼Œæ‚¨è¯•å›¾è§£æäº‹ä»¶çš„Â `body`Â ï¼Œå®ƒä¸åº”è¯¥åŒ…å«Â `DELETE`Â è¯·æ±‚çš„ä»»ä½•ä¿¡æ¯ã€‚
2.  Use theÂ `Key`Â parameter to specify theÂ `id`Â of the task to be deleted, instead of trying to scan theÂ `todos`Â table with aÂ `FilterExpression`. In the previous implementation, you were using aÂ `FilterExpression`, which is not needed since you have the ID of the task to delete. ä½¿ç”¨Â `Key`Â å‚æ•°æŒ‡å®šè¦åˆ é™¤çš„ä»»åŠ¡çš„Â `id`Â ï¼Œè€Œä¸æ˜¯å°è¯•ä½¿ç”¨Â `FilterExpression`Â æ‰«æÂ `todos`Â è¡¨ã€‚åœ¨ä¹‹å‰çš„å®ç°ä¸­ï¼Œæ‚¨ä½¿ç”¨äº†Â `FilterExpression`Â ï¼Œè¿™æ˜¯ä¸éœ€è¦çš„ï¼Œå› ä¸ºæ‚¨æœ‰è¦åˆ é™¤çš„ä»»åŠ¡çš„ IDã€‚
```js
case 'DELETE':
    const taskId = JSON.parse(event.pathParameters.taskId);
    const params = {
        TableName: tableName,
        Key: {
            id: taskId
        }
    };
    body = await dynamo.delete(params).promise();
    break;

```

For theÂ `PUT`Â request: å¯¹äºÂ `PUT`Â è¯·æ±‚ï¼š
1.  UseÂ `JSON.parse(event.body)`Â instead of trying to parse theÂ `Task`Â object from theÂ `event`Â object. In the previous implementation, there is no mention ofÂ `Task`Â object in theÂ `event`Â object.  ä½¿ç”¨Â `JSON.parse(event.body)`Â è€Œä¸æ˜¯å°è¯•ä»Â `event`Â å¯¹è±¡è§£æÂ `Task`Â å¯¹è±¡ã€‚åœ¨ä¹‹å‰çš„å®ç°ä¸­ï¼ŒÂ `event`Â å¯¹è±¡ä¸­å¹¶æ²¡æœ‰æåˆ°Â `Task`Â å¯¹è±¡ã€‚
2.  Use theÂ `Key`Â parameter to specify theÂ `id`Â of the task to be updated, and theÂ `UpdateExpression`Â parameter to update the relevant fields. In the previous implementation, you were trying to update the entireÂ `Task`Â object, which could result in unwanted side effects, such as overwriting existing fields.  ä½¿ç”¨Â `Key`Â å‚æ•°æŒ‡å®šè¦æ›´æ–°ä»»åŠ¡çš„Â `id`Â ï¼Œä½¿ç”¨Â `UpdateExpression`Â å‚æ•°æ›´æ–°ç›¸å…³å­—æ®µã€‚åœ¨ä¹‹å‰çš„å®ç°ä¸­ï¼Œæ‚¨è¯•å›¾æ›´æ–°æ•´ä¸ªÂ `Task`Â å¯¹è±¡ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´ä¸éœ€è¦çš„å‰¯ä½œç”¨ï¼Œä¾‹å¦‚è¦†ç›–ç°æœ‰å­—æ®µã€‚
```js
case 'PUT':
    const updatedFields = JSON.parse(event.body);
    const updatedValues = {};
    const updatedExpression = [];
    
    Object.keys(updatedFields).forEach(key => {
        updatedValues[`:${key}`] = updatedFields[key];
        updatedExpression.push(`${key} = :${key}`);
    });
    
    const updateParams = {
        TableName: tableName,
        Key: {
            id: updatedFields.id
        },
        UpdateExpression: `SET ${updatedExpression.join(', ')}`,
        ExpressionAttributeValues: updatedValues
    };

    body = await dynamo.update(updateParams).promise();
    break;

```

[HTTP Status Codes: A Complete List + Explanations](https://www.semrush.com/blog/http-status-codes/?kw=&cmp=AU_SRCH_DSA_Blog_EN&label=dsa_pagefeed&Network=g&Device=c&utm_content=622461199359&kwid=dsa-1754723155433&cmpid=18368690804&agpid=145078200167&BU=Core&extid=60162892887&adpos=&gad=1&gclid=CjwKCAjwxr2iBhBJEiwAdXECw25dPqMLy76lRrG6dJTDtebkUIM4ZBHQHfPtHjxe9KcGpgYI4STFTxoCS60QAvD_BwE#the-complete-list-of-http-status-codes) 
![](../img/fit5225-20230501.png)  
